{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MusicControlNet Training Test\n",
    "\n",
    "This notebook tests the training pipeline with a small subset of the NSynth dataset (100 samples total: 70 train, 15 validation, 15 test).\n",
    "\n",
    "## Objectives:\n",
    "- Load a small subset of NSynth dataset\n",
    "- Prepare audio data (convert to mel-spectrograms)\n",
    "- Generate text embeddings (using simple embeddings for testing)\n",
    "- Train the UNetMelGenerator model\n",
    "- Validate the training loop works correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets librosa matplotlib seaborn pandas numpy plotly soundfile tqdm torch torchaudio transformers -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our model\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from model import build_model\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Load Small Subset of NSynth Dataset\n",
    "\n",
    "We'll load exactly 100 samples from the dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading NSynth dataset from Parquet files...\")\n",
    "print(\"(Loading small subset for testing)\\n\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize HF API\n",
    "api = HfApi()\n",
    "\n",
    "# List all files in the repository\n",
    "print(\"Discovering dataset files...\")\n",
    "repo_files = list(api.list_repo_files(\"jg583/NSynth\", repo_type=\"dataset\"))\n",
    "\n",
    "# Find parquet files\n",
    "parquet_files = [f for f in repo_files if f.endswith('.parquet')]\n",
    "\n",
    "# Group by split\n",
    "train_parquet = [f for f in parquet_files if 'train' in f.lower()]\n",
    "valid_parquet = [f for f in parquet_files if 'valid' in f.lower() or 'validation' in f.lower()]\n",
    "test_parquet = [f for f in parquet_files if 'test' in f.lower()]\n",
    "\n",
    "print(f\"Found {len(parquet_files)} Parquet file(s)\")\n",
    "print(f\"  - Train: {len(train_parquet)} files\")\n",
    "print(f\"  - Valid: {len(valid_parquet)} files\")\n",
    "print(f\"  - Test: {len(test_parquet)} files\")\n",
    "\n",
    "# Create data files dict\n",
    "data_files = {}\n",
    "if train_parquet:\n",
    "    data_files[\"train\"] = [f\"hf://datasets/jg583/NSynth/{f}\" for f in train_parquet]\n",
    "if valid_parquet:\n",
    "    data_files[\"valid\"] = [f\"hf://datasets/jg583/NSynth/{f}\" for f in valid_parquet]\n",
    "if test_parquet:\n",
    "    data_files[\"test\"] = [f\"hf://datasets/jg583/NSynth/{f}\" for f in test_parquet]\n",
    "\n",
    "# Load dataset using streaming\n",
    "print(\"\\nLoading dataset using Parquet loader...\")\n",
    "dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=data_files,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded successfully!\")\n",
    "print(f\"Available splits: {list(dataset.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Extract 100 Samples and Split (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_samples(dataset_stream, n_samples):\n",
    "    \"\"\"Collect n samples from streaming dataset\"\"\"\n",
    "    samples = []\n",
    "    print(f\"Collecting {n_samples} samples...\")\n",
    "    for i, item in enumerate(tqdm(dataset_stream, total=n_samples)):\n",
    "        if i >= n_samples:\n",
    "            break\n",
    "        samples.append(item)\n",
    "    print(f\"Collected {len(samples)} samples\")\n",
    "    return samples\n",
    "\n",
    "# Collect 100 samples from train split\n",
    "print(\"Collecting 100 samples from train split...\")\n",
    "all_samples = collect_samples(dataset['train'], 100)\n",
    "\n",
    "# Split into 70/15/15\n",
    "train_samples = all_samples[:70]\n",
    "val_samples = all_samples[70:85]\n",
    "test_samples = all_samples[85:100]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATA SPLIT COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "print(f\"Test samples: {len(test_samples)}\")\n",
    "print(f\"Total: {len(all_samples)}\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample from training set:\")\n",
    "print(\"=\"*80)\n",
    "sample = train_samples[0]\n",
    "for key, value in sample.items():\n",
    "    if key != 'audio':\n",
    "        print(f\"{key:25s}: {value}\")\n",
    "    else:\n",
    "        if isinstance(value, dict) and 'array' in value:\n",
    "            print(f\"{key:25s}: [array of {len(value['array'])} samples at {value['sampling_rate']} Hz]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Audio Processing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-spectrogram parameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 256\n",
    "N_MELS = 128\n",
    "DURATION = 4.0  # seconds\n",
    "TARGET_LENGTH = int(SAMPLE_RATE * DURATION)  # 64,000 samples\n",
    "MEL_TIME_STEPS = int(TARGET_LENGTH / HOP_LENGTH)  # ~250 time steps\n",
    "\n",
    "print(\"Audio Processing Configuration:\")\n",
    "print(f\"  Sample Rate: {SAMPLE_RATE} Hz\")\n",
    "print(f\"  Duration: {DURATION} seconds\")\n",
    "print(f\"  Target Length: {TARGET_LENGTH} samples\")\n",
    "print(f\"  N_FFT: {N_FFT}\")\n",
    "print(f\"  Hop Length: {HOP_LENGTH}\")\n",
    "print(f\"  N_Mels: {N_MELS}\")\n",
    "print(f\"  Mel Time Steps: {MEL_TIME_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Create PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSynthMelDataset(Dataset):\n",
    "    \"\"\"NSynth dataset that converts audio to mel-spectrograms\"\"\"\n",
    "    \n",
    "    def __init__(self, samples, sample_rate=16000, n_mels=128, n_fft=1024, \n",
    "                 hop_length=256, duration=4.0):\n",
    "        self.samples = samples\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.target_length = int(sample_rate * duration)\n",
    "        \n",
    "        # Create simple text embeddings based on instrument family\n",
    "        self.family_to_idx = {}\n",
    "        families = list(set([s['instrument_family_str'] for s in samples]))\n",
    "        for i, family in enumerate(sorted(families)):\n",
    "            self.family_to_idx[family] = i\n",
    "        \n",
    "        print(f\"Dataset created with {len(samples)} samples\")\n",
    "        print(f\"Instrument families: {len(self.family_to_idx)}\")\n",
    "        print(f\"Families: {list(self.family_to_idx.keys())}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _process_audio(self, audio_array, sr):\n",
    "        \"\"\"Convert audio to mel-spectrogram\"\"\"\n",
    "        # Ensure audio_array is a numpy array\n",
    "        if not isinstance(audio_array, np.ndarray):\n",
    "            audio_array = np.array(audio_array, dtype=np.float32)\n",
    "        \n",
    "        # Ensure float type\n",
    "        if audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            audio_array = librosa.resample(audio_array, orig_sr=sr, target_sr=self.sample_rate)\n",
    "        \n",
    "        # Pad or trim to target length\n",
    "        if len(audio_array) < self.target_length:\n",
    "            audio_array = np.pad(audio_array, (0, self.target_length - len(audio_array)))\n",
    "        else:\n",
    "            audio_array = audio_array[:self.target_length]\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio_array,\n",
    "            sr=self.sample_rate,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            n_mels=self.n_mels,\n",
    "            fmin=0,\n",
    "            fmax=self.sample_rate // 2\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale (dB)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        mel_spec_normalized = (mel_spec_db + 80) / 80  # Assuming -80 dB is silence\n",
    "        mel_spec_normalized = np.clip(mel_spec_normalized, -1, 1)\n",
    "        \n",
    "        return mel_spec_normalized\n",
    "    \n",
    "    def _create_text_embedding(self, sample):\n",
    "        \"\"\"Create a simple text embedding from sample metadata\"\"\"\n",
    "        # Use one-hot encoding for instrument family\n",
    "        family_idx = self.family_to_idx[sample['instrument_family_str']]\n",
    "        n_families = len(self.family_to_idx)\n",
    "        \n",
    "        # Create a 512-dim embedding (to match expected input)\n",
    "        # First part: one-hot family encoding (padded)\n",
    "        # Second part: normalized pitch, velocity, and qualities\n",
    "        embedding = np.zeros(512, dtype=np.float32)\n",
    "        embedding[family_idx] = 1.0  # One-hot family\n",
    "        \n",
    "        # Add pitch and velocity info (normalized to [0, 1])\n",
    "        embedding[n_families] = sample['pitch'] / 127.0\n",
    "        embedding[n_families + 1] = sample['velocity'] / 127.0\n",
    "        \n",
    "        # Add source info (one-hot, 3 categories)\n",
    "        source_map = {'acoustic': 0, 'electronic': 1, 'synthetic': 2}\n",
    "        if sample['instrument_source_str'] in source_map:\n",
    "            source_idx = source_map[sample['instrument_source_str']]\n",
    "            embedding[n_families + 2 + source_idx] = 1.0\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Get audio - handle different possible formats\n",
    "        audio_data = sample['audio']\n",
    "        if isinstance(audio_data, dict):\n",
    "            audio_array = audio_data['array']\n",
    "            sr = audio_data['sampling_rate']\n",
    "        else:\n",
    "            # If it's not a dict, assume it's the array directly\n",
    "            audio_array = audio_data\n",
    "            sr = self.sample_rate\n",
    "        \n",
    "        # Process to mel-spectrogram\n",
    "        mel_spec = self._process_audio(audio_array, sr)\n",
    "        \n",
    "        # Create text embedding\n",
    "        text_emb = self._create_text_embedding(sample)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        # mel_spec: [n_mels, time] -> we'll use it as input and target\n",
    "        mel_tensor = torch.FloatTensor(mel_spec).unsqueeze(0)  # [1, n_mels, time]\n",
    "        text_tensor = torch.FloatTensor(text_emb)\n",
    "        \n",
    "        # Create noise for input (simple Gaussian noise)\n",
    "        noise = torch.randn_like(mel_tensor)\n",
    "        \n",
    "        # Input: concatenate mel + noise along channel dimension\n",
    "        input_tensor = torch.cat([mel_tensor, noise], dim=0)  # [2, n_mels, time]\n",
    "        \n",
    "        return {\n",
    "            'input': input_tensor,\n",
    "            'target': mel_tensor,\n",
    "            'text_emb': text_tensor,\n",
    "            'metadata': {\n",
    "                'note': sample['note'],\n",
    "                'instrument': sample['instrument_str'],\n",
    "                'family': sample['instrument_family_str'],\n",
    "                'pitch': sample['pitch'],\n",
    "                'velocity': sample['velocity']\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NSynthMelDataset(train_samples, sample_rate=SAMPLE_RATE, n_mels=N_MELS, \n",
    "                                 n_fft=N_FFT, hop_length=HOP_LENGTH, duration=DURATION)\n",
    "val_dataset = NSynthMelDataset(val_samples, sample_rate=SAMPLE_RATE, n_mels=N_MELS,\n",
    "                               n_fft=N_FFT, hop_length=HOP_LENGTH, duration=DURATION)\n",
    "test_dataset = NSynthMelDataset(test_samples, sample_rate=SAMPLE_RATE, n_mels=N_MELS,\n",
    "                                n_fft=N_FFT, hop_length=HOP_LENGTH, duration=DURATION)\n",
    "\n",
    "# Update val and test dataset family mappings\n",
    "val_dataset.family_to_idx = train_dataset.family_to_idx\n",
    "test_dataset.family_to_idx = train_dataset.family_to_idx\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASETS CREATED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Test Dataset Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a single sample\n",
    "print(\"Testing dataset output...\")\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(f\"\\nInput shape: {sample['input'].shape}\")  # Expected: [2, 128, ~250]\n",
    "print(f\"Target shape: {sample['target'].shape}\")  # Expected: [1, 128, ~250]\n",
    "print(f\"Text embedding shape: {sample['text_emb'].shape}\")  # Expected: [512]\n",
    "\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in sample['metadata'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Visualize mel-spectrogram\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Input (mel channel)\n",
    "axes[0].imshow(sample['input'][0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title('Input Mel-spectrogram')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Mel Frequency')\n",
    "\n",
    "# Input (noise channel)\n",
    "axes[1].imshow(sample['input'][1].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1].set_title('Input Noise')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Mel Frequency')\n",
    "\n",
    "# Target\n",
    "axes[2].imshow(sample['target'][0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[2].set_title('Target Mel-spectrogram')\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].set_ylabel('Mel Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Input: {batch['input'].shape}\")\n",
    "print(f\"  Target: {batch['target'].shape}\")\n",
    "print(f\"  Text embedding: {batch['text_emb'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "N_MELS = 128\n",
    "IN_CHANNELS = 2  # mel + noise\n",
    "BASE_CHANNELS = 32  # Reduced for faster training on small dataset\n",
    "CHANNEL_MULTS = (1, 2, 2, 4)  # Reduced complexity\n",
    "RAW_TEXT_EMB_DIM = 512\n",
    "TEXT_COND_DIM = 128  # Reduced\n",
    "\n",
    "print(\"Building model...\")\n",
    "gen, text_proj = build_model(\n",
    "    n_mels=N_MELS,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    base_channels=BASE_CHANNELS,\n",
    "    channel_mults=CHANNEL_MULTS,\n",
    "    raw_text_emb_dim=RAW_TEXT_EMB_DIM,\n",
    "    text_cond_dim=TEXT_COND_DIM\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "gen = gen.to(device)\n",
    "text_proj = text_proj.to(device)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Generator parameters: {count_parameters(gen):,}\")\n",
    "print(f\"Text projector parameters: {count_parameters(text_proj):,}\")\n",
    "print(f\"Total parameters: {count_parameters(gen) + count_parameters(text_proj):,}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 10. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing forward pass...\")\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(train_loader))\n",
    "x = batch['input'].to(device)\n",
    "target = batch['target'].to(device)\n",
    "text_emb = batch['text_emb'].to(device)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Target shape: {target.shape}\")\n",
    "print(f\"Text embedding shape: {text_emb.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    text_cond = text_proj(text_emb)\n",
    "    output = gen(x, text_cond)\n",
    "\n",
    "print(f\"\\nText condition shape: {text_cond.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\n✅ Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 11. Define Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "# Loss function (MSE for mel-spectrogram reconstruction)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(gen.parameters()) + list(text_proj.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=NUM_EPOCHS,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient Clipping: {GRAD_CLIP}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Scheduler: CosineAnnealingLR\")\n",
    "print(f\"  Loss: MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    gen.train()\n",
    "    text_proj.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Train]')\n",
    "    for batch in train_pbar:\n",
    "        x = batch['input'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "        text_emb = batch['text_emb'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        text_cond = text_proj(text_emb)\n",
    "        output = gen(x, text_cond)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(gen.parameters(), GRAD_CLIP)\n",
    "        torch.nn.utils.clip_grad_norm_(text_proj.parameters(), GRAD_CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Validation phase\n",
    "    gen.eval()\n",
    "    text_proj.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Val]')\n",
    "        for batch in val_pbar:\n",
    "            x = batch['input'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            text_emb = batch['text_emb'].to(device)\n",
    "            \n",
    "            text_cond = text_proj(text_emb)\n",
    "            output = gen(x, text_cond)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            val_losses.append(loss.item())\n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record history\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 13. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(epochs, history['lr'], 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Metrics:\")\n",
    "print(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Best Val Loss: {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss'])+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 14. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "gen.eval()\n",
    "text_proj.eval()\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        x = batch['input'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "        text_emb = batch['text_emb'].to(device)\n",
    "        \n",
    "        text_cond = text_proj(text_emb)\n",
    "        output = gen(x, text_cond)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Min Test Loss: {min(test_losses):.4f}\")\n",
    "print(f\"Max Test Loss: {max(test_losses):.4f}\")\n",
    "print(f\"Std Test Loss: {np.std(test_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 15. Visualize Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test batch for visualization\n",
    "test_batch = next(iter(test_loader))\n",
    "x_test = test_batch['input'].to(device)\n",
    "target_test = test_batch['target'].to(device)\n",
    "text_emb_test = test_batch['text_emb'].to(device)\n",
    "metadata_test = test_batch['metadata']\n",
    "\n",
    "# Generate predictions\n",
    "gen.eval()\n",
    "with torch.no_grad():\n",
    "    text_cond_test = text_proj(text_emb_test)\n",
    "    output_test = gen(x_test, text_cond_test)\n",
    "\n",
    "# Move to CPU for plotting\n",
    "x_cpu = x_test.cpu()\n",
    "target_cpu = target_test.cpu()\n",
    "output_cpu = output_test.cpu()\n",
    "\n",
    "# Plot first 4 samples in batch\n",
    "n_samples = min(4, len(x_cpu))\n",
    "fig, axes = plt.subplots(n_samples, 3, figsize=(15, 4*n_samples))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    if n_samples == 1:\n",
    "        ax_row = axes\n",
    "    else:\n",
    "        ax_row = axes[i]\n",
    "    \n",
    "    # Input mel-spectrogram\n",
    "    ax_row[0].imshow(x_cpu[i, 0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax_row[0].set_title(f'Input Mel\\n{metadata_test[\"family\"][i]}')\n",
    "    ax_row[0].set_ylabel('Mel Frequency')\n",
    "    \n",
    "    # Target mel-spectrogram\n",
    "    ax_row[1].imshow(target_cpu[i, 0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax_row[1].set_title(f'Target Mel\\nPitch: {metadata_test[\"pitch\"][i]}')\n",
    "    \n",
    "    # Generated mel-spectrogram\n",
    "    ax_row[2].imshow(output_cpu[i, 0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    mse = F.mse_loss(output_cpu[i], target_cpu[i]).item()\n",
    "    ax_row[2].set_title(f'Generated Mel\\nMSE: {mse:.4f}')\n",
    "    \n",
    "    if i == n_samples - 1:\n",
    "        for ax in ax_row:\n",
    "            ax.set_xlabel('Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## 16. Save Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'gen_state_dict': gen.state_dict(),\n",
    "    'text_proj_state_dict': text_proj.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'n_mels': N_MELS,\n",
    "        'in_channels': IN_CHANNELS,\n",
    "        'base_channels': BASE_CHANNELS,\n",
    "        'channel_mults': CHANNEL_MULTS,\n",
    "        'raw_text_emb_dim': RAW_TEXT_EMB_DIM,\n",
    "        'text_cond_dim': TEXT_COND_DIM,\n",
    "        'sample_rate': SAMPLE_RATE,\n",
    "        'n_fft': N_FFT,\n",
    "        'hop_length': HOP_LENGTH,\n",
    "        'duration': DURATION\n",
    "    }\n",
    "}\n",
    "\n",
    "checkpoint_path = 'model_checkpoint_test.pt'\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"✅ Model checkpoint saved to: {checkpoint_path}\")\n",
    "\n",
    "# Also save just the model weights\n",
    "torch.save({\n",
    "    'gen': gen.state_dict(),\n",
    "    'text_proj': text_proj.state_dict()\n",
    "}, 'model_weights_test.pt')\n",
    "print(f\"✅ Model weights saved to: model_weights_test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## 17. Summary\n",
    "\n",
    "### Training Test Results:\n",
    "- ✅ Successfully loaded 100 samples from NSynth dataset\n",
    "- ✅ Split data into 70/15/15 (train/val/test)\n",
    "- ✅ Converted audio to mel-spectrograms\n",
    "- ✅ Created simple text embeddings from metadata\n",
    "- ✅ Built UNetMelGenerator model\n",
    "- ✅ Trained for {NUM_EPOCHS} epochs\n",
    "- ✅ Model can generate mel-spectrograms conditioned on text\n",
    "\n",
    "### Next Steps:\n",
    "1. Scale up to full dataset\n",
    "2. Implement proper text encoder (CLAP/T5)\n",
    "3. Add diffusion noise schedule\n",
    "4. Implement proper evaluation metrics (FID, IS, etc.)\n",
    "5. Add mel-to-audio vocoder (HiFi-GAN, etc.)\n",
    "6. Experiment with different architectures and hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
